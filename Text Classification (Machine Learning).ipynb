{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "\n",
    "import random\n",
    "\n",
    "# WARMING UP EXERCISE: BUILD A GENDER CLASSIFIER\n",
    "\n",
    "names = ([(name, 'male') for name in names.words('male.txt')] +\n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "# Create random name list\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dudley', 'Joyan', 'Maud', 'Ainslie', 'Marius', 'Tobey', 'Urbain', 'Shara', 'Kathryn', 'Bernita']\n"
     ]
    }
   ],
   "source": [
    "# Show examples\n",
    "print([name for (name, gender) in names[:10]])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'h'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gender Feature function\n",
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}\n",
    "\n",
    "# Try gender_feature\n",
    "gender_features('Thinh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728\n"
     ]
    }
   ],
   "source": [
    "# Generate a training set\n",
    "featuresets = [(gender_features(n), g) for (n,g) in names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test classifier\n",
    "classifier.classify(gender_features('Adam'))\n",
    "\n",
    "# Test accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     34.3 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.4 : 1.0\n",
      "             last_letter = 'v'              male : female =     17.6 : 1.0\n",
      "             last_letter = 'f'              male : female =     17.4 : 1.0\n",
      "             last_letter = 'p'              male : female =     12.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Statistic on features\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781\n",
      "correct=female   guess=male     namme=Lust                          \n",
      "correct=female   guess=male     namme=Robinet                       \n",
      "correct=female   guess=male     namme=Roselyn                       \n",
      "correct=male     guess=female   namme=Abel                          \n",
      "correct=male     guess=female   namme=Benjie                        \n",
      "correct=male     guess=female   namme=Boyce                         \n",
      "correct=male     guess=female   namme=Dana                          \n",
      "correct=male     guess=female   namme=Darcy                         \n",
      "correct=male     guess=female   namme=Jodie                         \n",
      "correct=male     guess=female   namme=Kalil                         \n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# ERROR ANALYSIS TECHNIQUE\n",
    "# -------------------------------\n",
    "\n",
    "# Create name list\n",
    "train_names = names[1500:]\n",
    "devtest_names = names[500:1500]\n",
    "test_names = names[:500]\n",
    "\n",
    "# Create development set and test sest\n",
    "train_set = [(gender_features(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n",
    "test_set = [(gender_features(n), g) for (n,g) in test_names]\n",
    "\n",
    "# Train classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test accuracy\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))\n",
    "\n",
    "# Generate a list of ERRORs\n",
    "errors = []\n",
    "for (name, tag) in devtest_names:\n",
    "    guess = classifier.classify(gender_features(name))\n",
    "    if guess != tag:\n",
    "        errors.append((tag, guess, name))\n",
    "\n",
    "# Print out ERRORs list\n",
    "count = 0\n",
    "for (tag, guess, name) in sorted(errors[:10]): # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\n",
    "    print('correct=%-8s guess=%-8s namme=%-30s' % (tag, guess, name)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791\n"
     ]
    }
   ],
   "source": [
    "# Base on ERROR ANALYSIS, adjusting gender_features\n",
    "def gender_features(word):\n",
    "    return {'suffix1': word[-1:],\n",
    "            'suffix2' : word[-2:]}\n",
    "\n",
    "# Rebuild the classifier\n",
    "train_set = [(gender_features(n), g) for (n,g) in train_names]\n",
    "devtest_set = [(gender_features(n), g) for (n,g) in devtest_names]\n",
    "test_set = [(gender_features(n), g) for (n,g) in test_names]\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test accuracy\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))\n",
    "\n",
    "# => The accuracy increase thanks to adding more feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos', 'pos', 'neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "# DOCUMENT CLASSIFICATION\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "                for category in movie_reviews.categories()\n",
    "                for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Examples\n",
    "print([c for (f,c) in documents[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTOR\n",
    "\n",
    "# Find 2k most frequent words\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words()) # sorted\n",
    "word_features = list(all_words.keys())[:2000]\n",
    "\n",
    "# Generate features\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Example\n",
    "# print(document_features(movie_reviews.words('pos/cv957_8737.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81\n",
      "Most Informative Features\n",
      "           contains(ugh) = True              neg : pos    =      9.5 : 1.0\n",
      " contains(unimaginative) = True              neg : pos    =      8.2 : 1.0\n",
      "          contains(mena) = True              neg : pos    =      6.9 : 1.0\n",
      "        contains(shoddy) = True              neg : pos    =      6.9 : 1.0\n",
      "        contains(suvari) = True              neg : pos    =      6.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# TRAINING AND TESTING A CLASSIFIER FOR DOCUMENT CLASSIFICATION\n",
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# Find most informative features\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'he', 'the', 'n', 'on', 'ton', 'y', 'ty', 'nty', 'd', 'nd', 'and', 'ry', 'ury', 'id', 'aid', 'ay', 'day', 'an', 'ion', 'f', 'of', 's', \"'s\", \"a's\", 't', 'nt', 'ent', 'ary', 'ed', 'ced', '`', '``', 'o', 'no', 'ce', 'nce', \"'\", \"''\", 'at', 'hat', 'ny', 'any', 'es', 'ies', 'k', 'ok', 'ook', 'ace', '.', 'r', 'er', 'her', 'in', 'end', 'ts', 'nts', 'ity', 've', 'ive', 'ee', 'tee', ',', 'h', 'ch', 'ich', 'ad', 'had', 'l', 'll', 'all', 'ge', 'rge', 'ves', 'se', 'ise', 'ks', 'nks', 'a', 'ta', 'nta', 'or', 'for', 'ner', 'as', 'was', 'ted', 'ber', 'm', 'rm', 'erm', 'en', 'een', 'ged', 'by', 'ior', 'rt', 'urt', 'dge', 'od']\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# PART OF SPEECH TAGGING\n",
    "# ---------------------------------\n",
    "import nltk\n",
    "from  nltk.corpus import brown\n",
    "\n",
    "# Find the most common suffixes\n",
    "suffix_fdist = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "    word = word.lower()\n",
    "    suffix_fdist[word[-1:]] += 1\n",
    "    suffix_fdist[word[-2:]] += 1\n",
    "    suffix_fdist[word[-3:]] += 1\n",
    "\n",
    "common_suffixes = list(suffix_fdist.keys())[:100]\n",
    "print(common_suffixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a feature extractor\n",
    "def pos_features(word):\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith(%s)' % suffix] = word.lower().endswith(suffix)\n",
    "    return features\n",
    "\n",
    "# Note: word.lower().endswith(suffix) is a boolean\n",
    "'thinh'.lower().endswith('nh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5689706613625062\n"
     ]
    }
   ],
   "source": [
    "# Train a 'decision tree' classifier\n",
    "tagged_words = brown.tagged_words(categories='news')\n",
    "featuresets = [(pos_features(n), g) for (n,g) in tagged_words]\n",
    "\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "\n",
    "# Test accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNS\n",
      "VBN\n",
      "NN\n",
      "\n",
      "if endswith(the) == False: \n",
      "  if endswith(,) == False: \n",
      "    if endswith(s) == False: \n",
      "      if endswith(.) == False: \n",
      "        if endswith(of) == False: return '.'\n",
      "        if endswith(of) == True: return 'IN'\n",
      "      if endswith(.) == True: return '.'\n",
      "    if endswith(s) == True: \n",
      "      if endswith(was) == False: \n",
      "        if endswith(as) == False: return 'PP$'\n",
      "        if endswith(as) == True: return 'CS'\n",
      "      if endswith(was) == True: return 'BEDZ'\n",
      "  if endswith(,) == True: return ','\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try the classifier\n",
    "print(classifier.classify(pos_features('cats')))\n",
    "print(classifier.classify(pos_features('missed')))\n",
    "print(classifier.classify(pos_features('him')) + '\\n')\n",
    "\n",
    "# See the decision Tree\n",
    "print(classifier.pseudocode(depth=5))\n",
    "\n",
    "# NOTE: The classifer has low accuracy partly because it does not take into account the context of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix(1)': 'n', 'suffix(2)': 'on', 'suffix(3)': 'ion', 'prev-word': 'an'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a context-dependent feature extractor\n",
    "def pos_features(sentence, i):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "    return features\n",
    "\n",
    "# Try the new extractor\n",
    "pos_features(brown.sents()[0],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature sets\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "featuresets = []\n",
    "for tagged_sent in tagged_sents:\n",
    "    untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "    for i, (word, tag) in enumerate(tagged_sent):\n",
    "        featuresets.append((pos_features(untagged_sent, i), tag))\n",
    "        \n",
    "# NOTE: i represents the index using enumerate() method\n",
    "#       nltk.tag.untag feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7891596220785678\n"
     ]
    }
   ],
   "source": [
    "# Train the classifer again\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test the accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# NOTE: The accuracy increases thanks to introducing context, however the classifier\n",
    "# only check the actual previous words, not their tags. This can make our context-dependent feature less effective  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENCE CLASSIFICATION\n",
    "# Strategy: consecutive classification (greedy sequence classification)\n",
    "\n",
    "# Introduce tagged prev_word to feature extractor by adding history factor\n",
    "def pos_features(sentence, i, history):\n",
    "    features = {\"suffix(1)\": sentence[i][-1:],\n",
    "                \"suffix(2)\": sentence[i][-2:],\n",
    "                \"suffix(3)\": sentence[i][-3:]}\n",
    "    \n",
    "    if i == 0:\n",
    "        features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-tag\"] = \"<START>\"\n",
    "    else:\n",
    "        features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-tag\"] = history[i-1]\n",
    "    return features\n",
    "\n",
    "class ConsecutivePosTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = pos_features(untagged_sent, i, history)\n",
    "                train_set.append((featureset,tag))\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "    \n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = pos_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7980528511821975\n"
     ]
    }
   ],
   "source": [
    "# Train the new classifier\n",
    "tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(tagged_sents) * 0.1)\n",
    "train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]\n",
    "tagger = ConsecutivePosTagger(train_sents)\n",
    "\n",
    "# Test the accuracy\n",
    "print(tagger.evaluate(test_sents))\n",
    "\n",
    "# NOTE: the accuracy increases thanks to adding history factor. However, the limitation of this approach is that when we label,\n",
    "# a word, we cannot go back to relabel it when we have more evidence to make better decision. \n",
    "# => OPTION 1: Consider a transformational strategy (checking the Brill tagger)\n",
    "# => OPTION 2: HIDDEN MARKOV MODELS \n",
    "#              assign scores to all possible sequences of POS tags, and choose the highest (generate probability distribution) \n",
    "#              use dynamic programming to reduce complexity \n",
    "#              (checking Maximum Entropy Markov Models & Linear-Chain Conditional Random Field Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# SENTENCE SEGMENTATION TECHNIQUE\n",
    "# -------------------------------\n",
    "\n",
    "# Generating necessary features\n",
    "sents = nltk.corpus.treebank_raw.sents()\n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in nltk.corpus.treebank_raw.sents():\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)\n",
    "\n",
    "# NOTE: tokens is a merged list of individual sentences\n",
    "#       boundaries is a set containing indexes of all sentence-boundary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation Feature Extractor\n",
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prevword': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}\n",
    "\n",
    "# Generate a feature set\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "                    for i in range(1, len(tokens)-1)\n",
    "                    if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.936026936026936"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the punctuatioin classifier\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test the accuracy\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                prevword = 'mr'            False : True   =    136.9 : 1.0\n",
      "                prevword = '3'             False : True   =     43.2 : 1.0\n",
      "                prevword = '7'             False : True   =     40.4 : 1.0\n",
      "                prevword = '2'             False : True   =     37.7 : 1.0\n",
      "                prevword = '6'             False : True   =     25.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the punctuatioin classifier to build a sentence segmenter\n",
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nice', 'to', 'meet', 'you', '!', 'my', 'name', 'is', 'Thinh', '.', 'Can', 'we', 'be', 'friends', '?']\n",
      "[['Nice', 'to', 'meet', 'you', '!'], ['my', 'name', 'is', 'Thinh', '.'], ['Can', 'we', 'be', 'friends', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Test sentence segmenter\n",
    "# text = \"Nice to meet you ! my name is Thinh . Can we be friends ?\"\n",
    "text = [\"Nice\", \"to\", \"meet\", \"you\", \"!\", \"my\", \"name\", \"is\", \"Thinh\", \".\", \"Can\", \"we\", \"be\", \"friends\", \"?\"]\n",
    "print(text)\n",
    "segments = segment_sentences(text)\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nice to meet you !', 'my name is Thinh .', 'Can we be friends ?']\n"
     ]
    }
   ],
   "source": [
    "# Recreate sentece from BOW\n",
    "s = ' '\n",
    "sentences = [s.join(segment) for segment in segments]\n",
    "print(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
